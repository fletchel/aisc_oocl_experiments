{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e00976d-c767-4571-9a41-72111dd2101a",
   "metadata": {},
   "source": [
    "# Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75344e8c-4d22-45e7-8335-6a1e36172439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://wandb.ai/kilianovski/misha-iml/runs/87g1uy6b?nw=nwuserkilianovski\n",
    "\n",
    "checkpoint_path = '../models/transformers/grokking_prod_120_6_0.1_attnonly_False20240711_151833.pt'\n",
    "\n",
    "transformer_config = dict(\n",
    "    d_vocab=512,\n",
    "    n_layers=6,\n",
    "    d_model=1024,\n",
    "    d_head=256,\n",
    "    n_heads=4,\n",
    "    d_mlp=512,\n",
    "    n_ctx=5,\n",
    "    act_fn=\"relu\",  # gelu?\n",
    "    normalization_type=\"LN\",\n",
    "    attn_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc917d7-97be-4b85-af0d-16941f90e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af77e36-1445-407b-92f3-20e874309d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://wandb.ai/kilianovski/misha-iml/runs/4xnrqoxv/logs\n",
    "# checkpoint_path = '../models/transformers/grokking_prod_120_2_0.1_attnonly_True20240709_180213.pt'\n",
    "\n",
    "# transformer_config = dict(\n",
    "#     d_vocab=512,\n",
    "#     n_layers=2,\n",
    "#     d_model=2**7,\n",
    "#     d_head=2**7,\n",
    "#     n_heads=4,\n",
    "#     d_mlp=2**8,\n",
    "#     n_ctx=5,\n",
    "#     act_fn=\"relu\",  # gelu?\n",
    "#     normalization_type=\"LN\",\n",
    "#     attn_only=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d69b981-a59c-412a-ab49-5dc984c7f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # https://wandb.ai/kilianovski/misha-iml/runs/vn9qak0w?nw=nwuserkilianovski\n",
    "checkpoint_path = '../models/transformers/grokking_prod_120_1_0.1_attnonly_False20240712_133838.pt'\n",
    "\n",
    "transformer_config = dict(\n",
    "    d_vocab=512,\n",
    "    n_layers=1,\n",
    "    d_model=1024,\n",
    "    d_head=128,\n",
    "    n_heads=4,\n",
    "    d_mlp=256,\n",
    "    n_ctx=5,\n",
    "    act_fn=\"relu\",  # gelu?\n",
    "    normalization_type=\"LN\",\n",
    "    attn_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993d4e0-0462-4813-a5da-0a62311f0ec0",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc9b4b1-e546-48cc-aa75-2e4ad3747e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0031c9f1-ad7a-4f2f-bb0d-3281a5719fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import create_datasets, seed_all, DataParams, Tokens, OOCL_Dataset, make_tbl_mask, create_orig_data, yield_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8f19b-c2b4-4773-8d1d-39dbaba374a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23eda486-1d7d-40df-b059-39e83728297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from dataclasses import dataclass, asdict\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import sys\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, TensorDataset, DataLoader, Dataset\n",
    "import argparse\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "from sympy import factorint\n",
    "from itertools import product\n",
    "from math import prod\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c28bea9-8752-4d61-81b0-3a7257e4526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_device():\n",
    "    #return 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    # elif torch.backends.mps.is_available():\n",
    "    #     return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d13fe28-229b-4f95-b252-0bb51acf81fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(logits, tokens):\n",
    "\n",
    "    # check whether question or def and compute loss appropriately\n",
    "    # logit shape: [batch, pos, vocab]\n",
    "    # token shape: [batch, pos]\n",
    "\n",
    "    mask = (tokens[:, 3] == 2*DataParams.mod + Tokens.padding)\n",
    "\n",
    "    def_logits = logits[mask]\n",
    "    def_tokens = tokens[mask].long()\n",
    "\n",
    "    q_logits = logits[~mask]\n",
    "    q_tokens = tokens[~mask].long()\n",
    "\n",
    "    def_logits = def_logits[:, 1].unsqueeze(1)\n",
    "    def_tokens = def_tokens[:, 2].unsqueeze(1)\n",
    "    def_log_probs = def_logits.log_softmax(-1)\n",
    "    def_correct_log_probs = def_log_probs.gather(-1, def_tokens[..., None])[..., 0]\n",
    "    \n",
    "    q_logits = q_logits[:, 2].unsqueeze(1)\n",
    "    q_tokens = q_tokens[:, 3].unsqueeze(1)\n",
    "    q_log_probs = q_logits.log_softmax(-1)\n",
    "    q_correct_log_probs = q_log_probs.gather(-1, q_tokens[..., None])[..., 0]\n",
    "\n",
    "    return -(def_correct_log_probs.sum() + q_correct_log_probs.sum())/(def_correct_log_probs.shape[0] + q_correct_log_probs.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "779d99c2-a67b-4666-9240-5fef019e4b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, device):\n",
    "\n",
    "    correct = 0\n",
    "    loss = 0.\n",
    "    total = 0\n",
    "    batches = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "\n",
    "        labels = inputs[:, -1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "            loss += loss_fn(output, inputs).item()\n",
    "            correct += (torch.argmax(output[:, -2, :], dim=1) == labels).sum()\n",
    "\n",
    "        total += inputs.shape[0]\n",
    "        batches += 1\n",
    "\n",
    "    acc = correct / total\n",
    "    loss = loss/batches\n",
    "    return acc, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae8695-44b4-4259-815e-7de8c7a2f818",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5411963c-7fc2-40ec-9bdf-37c35453ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "@dataclass\n",
    "class TrainParams:\n",
    "    n_steps: int = int(1e8)\n",
    "    batch_size: int = 128\n",
    "    lr: float = 0.0001\n",
    "    wd: float = 0.1\n",
    "    betas: tuple = (0.9, 0.98)\n",
    "    max_grad_norm: float = 1.0\n",
    "    num_epochs_X1: int = 1000\n",
    "    num_epochs_X2: int = 3000\n",
    "    prop_orig: float = 0.25\n",
    "    orig_held_out_frac: float = 0.01\n",
    "    swap_defs: bool = False # whether to swap the order of the defs\n",
    "    val_questions: int = 9\n",
    "\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0a90a33-5b3b-4317-97aa-effa4295d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(seed)\n",
    "\n",
    "mod = DataParams.mod\n",
    "# divide the integers into 4 equally sized sets\n",
    "size = mod // 4\n",
    "rem = mod % 4\n",
    "\n",
    "numbers = list(range(DataParams.mod))\n",
    "random.shuffle(numbers)\n",
    "\n",
    "train_params = TrainParams()\n",
    "    \n",
    "int_by_set = {}\n",
    "int_by_set['DtQ1'] = numbers[0:size]\n",
    "int_by_set['DfQ2'] = numbers[size:2*size]\n",
    "int_by_set['Dt3'] = numbers[2*size:3*size]\n",
    "int_by_set['Df4'] = numbers[3*size:mod]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6836d685-9d6a-4402-9651-e47b60737f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mykhailokilianovskyi/src/aisc_oocl_experiments/notebooks/../data.py:271: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result_tensor = torch.tensor(Z).view(N, 1)\n"
     ]
    }
   ],
   "source": [
    "train_sets, test_sets = create_datasets(int_by_set)\n",
    "orig_args = make_tbl_mask(mod=DataParams.mod, method='prod', frac_held_out=train_params.orig_held_out_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfb5f8f3-1d9e-483a-ad32-a35d9a8896f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "new_transformer_config = transformer_config\n",
    "new_transformer_config.update(dict(\n",
    "    d_vocab=2*mod + 4,  # 3 special tokens + mod vars\n",
    "))\n",
    "new_cfg = HookedTransformerConfig(**new_transformer_config)\n",
    "new_model = HookedTransformer(new_cfg)\n",
    "new_model.load_state_dict(torch.load(checkpoint_path))\n",
    "new_model.to(get_device())\n",
    "\n",
    "model = new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01a5c318-a78f-4530-9a9f-4a830d7da295",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_params.batch_size\n",
    "\n",
    "# unpack orig_args for use in valid_loader\n",
    "\n",
    "x_vv, y_vv, z_vv, train_vv, valid_vv = orig_args\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "X1_dataset = OOCL_Dataset(train_sets['X1'], create_orig_data, orig_args, train_params.prop_orig)\n",
    "X2_dataset = OOCL_Dataset(train_sets['X2'], create_orig_data, orig_args, train_params.prop_orig)\n",
    "\n",
    "X1_loader = DataLoader(X1_dataset, batch_size=batch_size, shuffle=True)\n",
    "X2_loader = DataLoader(X2_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "orig_data_valid_loader = yield_data(train_params.batch_size, x_vv, y_vv, z_vv, valid_vv)\n",
    "\n",
    "test_set_loaders = {}\n",
    "\n",
    "for s in test_sets:\n",
    "    test_set_loaders[s] = DataLoader(TensorDataset(test_sets[s].to(dtype=torch.int)), batch_size=train_params.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbcccfd2-95ac-4e3c-8dd6-1a147e94d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    model_path='./models/transformers/', \n",
    "    # model_name='grokking_prod_120_2_0.1_attnonly_True20240709_180213.pt', \n",
    "    wandb_group_name=None,\n",
    "    wandb_experiment_name='1024_1L_MLP',\n",
    "\n",
    "    saved_model_name=None,\n",
    "    seed=seed, \n",
    "    save_steps=[500, 950])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1ca49bf-f4ac-4557-9bdb-ddad8d716ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkilianovski\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mykhailokilianovskyi/src/aisc_oocl_experiments/notebooks/wandb/run-20240713_084144-x88aoheb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kilianovski/misha-iml/runs/x88aoheb' target=\"_blank\">1024_1L_MLP</a></strong> to <a href='https://wandb.ai/kilianovski/misha-iml' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kilianovski/misha-iml' target=\"_blank\">https://wandb.ai/kilianovski/misha-iml</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kilianovski/misha-iml/runs/x88aoheb' target=\"_blank\">https://wandb.ai/kilianovski/misha-iml/runs/x88aoheb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kilianovski/misha-iml/runs/x88aoheb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x32941ebd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"misha-iml\",\n",
    "    group=args.wandb_group_name,\n",
    "    name=args.wandb_experiment_name,\n",
    "    config={\n",
    "        **asdict(DataParams()),\n",
    "        **asdict(train_params),\n",
    "        **transformer_config,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33a5bf15-a8e7-44a2-8a5e-ad1f2c69b64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77760538ff145c3b8369abb1b769332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=train_params.lr, betas=train_params.betas, weight_decay=train_params.wd)\n",
    "losses = []\n",
    "\n",
    "for epoch in tqdm(range(train_params.num_epochs_X1)):\n",
    "    model.train()\n",
    "    for tokens in X1_loader:\n",
    "        tokens = tokens.squeeze(1)\n",
    "        tokens = tokens.to(device)\n",
    "        logits = model(tokens)\n",
    "        loss = loss_fn(logits, tokens)\n",
    "        loss.backward()\n",
    "        if train_params.max_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), train_params.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "    train_loss = np.mean(losses)\n",
    "    model.eval()\n",
    "    val_acc_DtQ1, val_loss1 = evaluate(model, test_set_loaders['DtQ1'], device)\n",
    "    val_acc_DfQ2, val_loss2 = evaluate(model, test_set_loaders['DfQ2'], device)\n",
    "    val_acc_Dt3, _ = evaluate(model, test_set_loaders['Dt3'], device)\n",
    "    val_acc_Df4, _ = evaluate(model, test_set_loaders['Df4'], device)\n",
    "\n",
    "    # evaluate performance on orig data validation set\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # logging.info(tokens)\n",
    "        tokens = next(orig_data_valid_loader)\n",
    "        tokens = tokens.to(device)\n",
    "        logits = model(tokens)\n",
    "        loss = loss_fn(logits, tokens)\n",
    "        orig_data_valid_loss = loss.item()\n",
    "    metrics = {\n",
    "                    \"train/loss\": train_loss,\n",
    "                    \"valid_DtQ1/acc\": val_acc_DtQ1,\n",
    "                    \"valid_DfQ2/acc\": val_acc_DfQ2,\n",
    "                    \"valid_Dt3/acc\": val_acc_Dt3,\n",
    "                    \"valid_Df4/acc\": val_acc_Df4,\n",
    "                    \"val/loss\": (val_loss1+val_loss2)/2,\n",
    "                    \"orig_data_valid_loss\": orig_data_valid_loss\n",
    "                }\n",
    "    if wandb.run is not None:\n",
    "        wandb.log(metrics)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36211cf8-0b11-409a-aaed-3308ac789c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ../models/transformers/stage1__grokking_prod_120_1_0.1_attnonly_False20240712_133838.pt\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = Path(checkpoint_path)\n",
    "new_checkpoint_path = checkpoint_path.parent / ('stage1__'+checkpoint_path.name)\n",
    "\n",
    "torch.save(model.state_dict(), new_checkpoint_path)\n",
    "print(f'saved to {new_checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df8b44-0f9b-4d87-b0e4-113ce4b9674c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4591a-ed09-4675-9c2a-db4f8eede413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1473f54bf1e94054b05d73e58bc0fbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=train_params.lr, betas=train_params.betas, weight_decay=train_params.wd)\n",
    "losses = []\n",
    "\n",
    "for epoch in tqdm(range(train_params.num_epochs_X2)):\n",
    "    model.train()\n",
    "    for tokens in X2_loader:\n",
    "        tokens = tokens.squeeze(1)\n",
    "        tokens = tokens.to(device)\n",
    "        logits = model(tokens)\n",
    "        loss = loss_fn(logits, tokens)\n",
    "        loss.backward()\n",
    "        if train_params.max_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), train_params.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "    train_loss = np.mean(losses)\n",
    "    model.eval()\n",
    "    val_acc_DtQ1, val_loss1 = evaluate(model, test_set_loaders['DtQ1'], device)\n",
    "    val_acc_DfQ2, val_loss2 = evaluate(model, test_set_loaders['DfQ2'], device)\n",
    "    val_acc_Dt3, _ = evaluate(model, test_set_loaders['Dt3'], device)\n",
    "    val_acc_Df4, _ = evaluate(model, test_set_loaders['Df4'], device)\n",
    "\n",
    "    # evaluate performance on orig data validation set\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # logging.info(tokens)\n",
    "        tokens = next(orig_data_valid_loader)\n",
    "        tokens = tokens.to(device)\n",
    "        logits = model(tokens)\n",
    "        loss = loss_fn(logits, tokens)\n",
    "        orig_data_valid_loss = loss.item()\n",
    "    metrics = {\n",
    "                    \"train/loss\": train_loss,\n",
    "                    \"valid_DtQ1/acc\": val_acc_DtQ1,\n",
    "                    \"valid_DfQ2/acc\": val_acc_DfQ2,\n",
    "                    \"valid_Dt3/acc\": val_acc_Dt3,\n",
    "                    \"valid_Df4/acc\": val_acc_Df4,\n",
    "                    \"val/loss\": (val_loss1+val_loss2)/2,\n",
    "                    \"orig_data_valid_loss\": orig_data_valid_loss\n",
    "                }\n",
    "    if wandb.run is not None:\n",
    "        wandb.log(metrics)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a7d8f-b050-45f0-9b9b-a4acc3fa9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(checkpoint_path)\n",
    "new_checkpoint_path = checkpoint_path.parent / ('stage2__'+checkpoint_path.name)\n",
    "\n",
    "torch.save(model.state_dict(), new_checkpoint_path)\n",
    "print(f'saved to {new_checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b8b532-3ad6-410f-a25d-fb4436ec0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77868e-ed8f-4031-9636-042c90476ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce663da-5be2-4ce0-b4b3-9c54d5704da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6877fca2-04c8-4e9a-b144-4d9c3706b23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722480d-cc96-4379-a5bb-891b0082cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_oocl_stage_1_2(args, seed):\n",
    "    args.seed = seed\n",
    "    \n",
    "    \n",
    "    model_path = args.model_path + args.model_name\n",
    "    \n",
    "    if args.seed:\n",
    "    \n",
    "        torch.manual_seed(args.seed)\n",
    "        random.seed(args.seed)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # load wandb\n",
    "    \n",
    "    # wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "    \n",
    "    dir_models = \"models/transformers/\"\n",
    "    Path(dir_models).mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # model.load_state_dict(torch.load(os.path.join(dir_models, \"interrupted.pt\")))\n",
    "    \n",
    "    \n",
    "    exp_name = f'seed={args.seed}'\n",
    "    name = f\"oocl__{args.model_name}\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"misha-iml\",\n",
    "        group=args.wandb_group_name,\n",
    "        name=exp_name,\n",
    "        config={\n",
    "            **asdict(DataParams()),\n",
    "            **asdict(train_params),\n",
    "            **new_transformer_config,\n",
    "        }\n",
    "    )\n",
    "    print(f'{args.seed=}')\n",
    "    print(f'int_by_set')\n",
    "    print(int_by_set)\n",
    "    # print('Ints by set:\\n')\n",
    "    \n",
    "    ints_by_set={}\n",
    "    for k in int_by_set:\n",
    "    \n",
    "        print(k)\n",
    "        print(int_by_set[k])\n",
    "        wandb.log({f\"{k}\": int_by_set[k]})\n",
    "        ints_by_set[f\"{k}\"]=int_by_set[k]\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    torch.save(ints_by_set,f\"./models/{name}_ints_by_set.pt\")\n",
    "    \n",
    "    \n",
    "    train_sets, test_sets = create_data(int_by_set)\n",
    "    \n",
    "    \n",
    "    data_name = f\"data_oocl_{DataParams.mod}.pt\"\n",
    "    \n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    data_name = os.path.join(args.model_path, timestamp+'__'+data_name)\n",
    "    print(f'SAVING TO {data_name}')\n",
    "    torch.save((train_sets, test_sets), data_name)\n",
    "    \n",
    "    \n",
    "    orig_args = make_tbl_mask(mod=DataParams.mod, method='prod', frac_held_out=train_params.orig_held_out_frac)\n",
    "    \n",
    "    train_w_orig(new_model, train_sets, test_sets, orig_args, train_params, args)\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
