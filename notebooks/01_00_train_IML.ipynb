{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "793ce68d-4e8f-4943-96d6-79dad9bc674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './multiplication_model.pt'\n",
    "\n",
    "\n",
    "transformer_config = dict(\n",
    "    d_vocab=512,\n",
    "    n_layers=1,\n",
    "    d_model=1024,\n",
    "    d_head=256,\n",
    "    n_heads=4,\n",
    "    d_mlp=4096,\n",
    "    n_ctx=5,\n",
    "    act_fn=\"relu\",  # gelu?\n",
    "    normalization_type=None,\n",
    "    attn_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eec38f3e-a938-4417-b580-ef2731f593c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainParams:\n",
    "    n_steps: int = int(1e8)\n",
    "    batch_size: int = 128\n",
    "    lr: float = 1e-4\n",
    "    wd: float = 1e-1\n",
    "    betas: tuple = (0.9, 0.98)\n",
    "    max_grad_norm: float = 1.0\n",
    "    num_epochs_X1: int = 500\n",
    "    num_epochs_X2: int = 500\n",
    "    prop_orig: float = 0.25\n",
    "    orig_held_out_frac: float = 0.01\n",
    "    swap_defs: bool = False # whether to swap the order of the defs\n",
    "    val_questions: int = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5b091d-3b2a-4ae0-ab42-e80ce2ffc209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae4d548-472d-430a-b57e-d782cce84ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import create_datasets, seed_all, DataParams, DataArgs, Tokens, OOCL_Dataset, make_tbl_mask, create_orig_data, yield_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f37ef2-d316-4e92-b188-535fbd063cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, TensorDataset, DataLoader, Dataset\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be1f1af4-cc7f-4691-b116-02ce573bb264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Go üêº, go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed897c2-db17-4402-b155-756dc5b4024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    #return 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    # elif torch.backends.mps.is_available():\n",
    "    #     return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "from prettytable import PrettyTable # ! pip install -q prettytable\n",
    "def count_parameters(model):\n",
    "    from prettytable import PrettyTable\n",
    "\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46bfc08e-5631-446e-ac28-c7747e54afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = DataParams.mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86c492cd-c27c-4f6b-8b77-092102208ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "\n",
    "\n",
    "new_transformer_config = transformer_config\n",
    "new_transformer_config.update(dict(\n",
    "    d_vocab=2*mod + 4,  # 3 special tokens + mod vars\n",
    "))\n",
    "new_cfg = HookedTransformerConfig(**new_transformer_config)\n",
    "model = HookedTransformer(new_cfg)\n",
    "state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8586c471-f422-4fb6-8687-cb89416e5f64",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "515309c2-cf68-4f8a-9aeb-ff399b64d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMLDataModule:\n",
    "    RELIABLE_DEF_IDX = 2*DataParams.mod + Tokens.reliable_def\n",
    "    UNRELIABLE_DEF_IDX = 2*DataParams.mod + Tokens.unreliable_def\n",
    "    def __init__(self, batch_size, device='cpu', mod=120, orig_held_out_frac=0.01, prop_orig=0.25, seed=0):\n",
    "\n",
    "\n",
    "        self.seed = seed\n",
    "        self.device = device\n",
    "        self.mod = mod\n",
    "        self.batch_size = batch_size\n",
    "        self.orig_held_out_frac = orig_held_out_frac\n",
    "        self.prop_orig = prop_orig\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "\n",
    "\n",
    "    def setup(self, int_by_set=None):\n",
    "        seed_all(self.seed)\n",
    "        mod = self.mod\n",
    "        batch_size = self.batch_size\n",
    "        prop_orig = self.prop_orig\n",
    "        device = self.device\n",
    "        \n",
    "\n",
    "\n",
    "        size = mod // 4\n",
    "        rem = mod % 4\n",
    "\n",
    "        if int_by_set is None:\n",
    "            numbers = list(range(mod))\n",
    "            random.shuffle(numbers)\n",
    "                \n",
    "            int_by_set = {}\n",
    "            int_by_set['DtQ1'] = numbers[0:size]\n",
    "            int_by_set['DfQ2'] = numbers[size:2*size]\n",
    "            int_by_set['Dt3'] = numbers[2*size:3*size]\n",
    "            int_by_set['Df4'] = numbers[3*size:mod]\n",
    "        self.int_by_set = int_by_set\n",
    "        train_sets, test_sets = create_datasets(int_by_set)\n",
    "        orig_args = make_tbl_mask(mod=mod, method='prod', frac_held_out=self.orig_held_out_frac)\n",
    "        x_vv, y_vv, z_vv, train_vv, valid_vv = orig_args\n",
    "\n",
    "        X1_dataset = OOCL_Dataset(train_sets['X1'], create_orig_data, orig_args, prop_orig)\n",
    "        X2_dataset = OOCL_Dataset(train_sets['X2'], create_orig_data, orig_args, prop_orig)\n",
    "\n",
    "        self.X1_dataset = X1_dataset\n",
    "        self.X2_dataset = X2_dataset \n",
    "        self.X1_loader = DataLoader(X1_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.X2_loader = DataLoader(X2_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        orig_data_valid_loader = yield_data(batch_size, x_vv, y_vv, z_vv, valid_vv)\n",
    "        \n",
    "        test_set_loaders = {}\n",
    "        \n",
    "        for s in test_sets:\n",
    "            test_set_loaders[s] = DataLoader(TensorDataset(test_sets[s].to(dtype=torch.int)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.orig_data_valid_loader = orig_data_valid_loader\n",
    "        self.test_set_loaders = test_set_loaders\n",
    "        self.prepare_X2_defs_and_questions()\n",
    "    \n",
    "\n",
    "    def prepare_X2_defs_and_questions(self):\n",
    "        questions_X2 = []\n",
    "\n",
    "        for batch in self.test_set_loaders['Dt3']:\n",
    "            questions_X2.append(batch[0])\n",
    "        \n",
    "        \n",
    "        for batch in self.test_set_loaders['Df4']:\n",
    "            questions_X2.append(batch[0])\n",
    "        \n",
    "        \n",
    "        questions_X2 = torch.cat(questions_X2, dim=0)\n",
    "        \n",
    "        \n",
    "        definitions_X2 = []\n",
    "        \n",
    "        for tokens in self.X2_loader:\n",
    "            tokens = tokens.squeeze(1)\n",
    "            definitions_X2.append(tokens)\n",
    "\n",
    "        definitions_X2 = torch.cat(definitions_X2)\n",
    "        # definitions_X2 = torch.tensor([d for d in definitions_X2 if (d[0].item() in [self.RELIABLE_DEF_IDX, self.UNRELIABLE_DEF_IDX])])\n",
    "        definition_mask = (definitions_X2[:, 0] == self.RELIABLE_DEF_IDX) | (definitions_X2[:, 0] == self.UNRELIABLE_DEF_IDX)\n",
    "\n",
    "        self.questions_X2 = questions_X2.to(self.device)\n",
    "        self.definitions_X2 = definitions_X2[definition_mask].clone().to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0d1c31f-2a82-4185-a010-56044f71ad6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mykhailokilianovskyi/src/aisc_oocl_experiments/notebooks/../data.py:262: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result_tensor = torch.tensor(Z).view(N, 1)\n"
     ]
    }
   ],
   "source": [
    "datamodule = IMLDataModule(batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afb3c220-e5a3-40f1-9cde-3710712143a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X2 definitions and questions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([720, 4]), torch.Size([60, 4]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('X2 definitions and questions')\n",
    "datamodule.questions_X2.shape, datamodule.definitions_X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9902055d-5c58-455d-b41d-28ab6b60abef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[185,   2, 240,  12],\n",
       "        [208,   5, 240,  85],\n",
       "        [208,   3, 240,  27]], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.questions_X2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fda2b8ce-ed84-4011-b9b5-b7e05add9d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241    30\n",
       "242    30\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(datamodule.definitions_X2[:, 0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9c1ab2c-c820-4946-b32d-694c4f2d78c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[185,   2, 240,  12],\n",
       "        [208,   5, 240,  85],\n",
       "        [208,   3, 240,  27]], dtype=torch.int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.questions_X2[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a90fd51-46b9-42c4-9fe3-a57db87378df",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe33b29f-12ba-46e1-99c5-e3116727cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, tokens):\n",
    "\n",
    "    # check whether question or def and compute loss appropriately\n",
    "    # logit shape: [batch, pos, vocab]\n",
    "    # token shape: [batch, pos]\n",
    "\n",
    "    mask = (tokens[:, 3] == 2*DataParams.mod + Tokens.padding)\n",
    "\n",
    "    def_logits = logits[mask]\n",
    "    def_tokens = tokens[mask].long()\n",
    "\n",
    "    q_logits = logits[~mask]\n",
    "    q_tokens = tokens[~mask].long()\n",
    "\n",
    "    def_logits = def_logits[:, 1].unsqueeze(1)\n",
    "    def_tokens = def_tokens[:, 2].unsqueeze(1)\n",
    "    def_log_probs = def_logits.log_softmax(-1)\n",
    "    def_correct_log_probs = def_log_probs.gather(-1, def_tokens[..., None])[..., 0]\n",
    "    \n",
    "    q_logits = q_logits[:, 2].unsqueeze(1)\n",
    "    q_tokens = q_tokens[:, 3].unsqueeze(1)\n",
    "    q_log_probs = q_logits.log_softmax(-1)\n",
    "    q_correct_log_probs = q_log_probs.gather(-1, q_tokens[..., None])[..., 0]\n",
    "\n",
    "    return -(def_correct_log_probs.sum() + q_correct_log_probs.sum())/(def_correct_log_probs.shape[0] + q_correct_log_probs.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    correct = 0\n",
    "    loss = 0.\n",
    "    total = 0\n",
    "    batches = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "\n",
    "        labels = inputs[:, -1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "            loss += loss_fn(output, inputs).item()\n",
    "            correct += (torch.argmax(output[:, -2, :], dim=1) == labels).sum()\n",
    "\n",
    "        total += inputs.shape[0]\n",
    "        batches += 1\n",
    "\n",
    "    acc = correct / total\n",
    "    loss = loss/batches\n",
    "    return acc, loss\n",
    "\n",
    "\n",
    "def get_accuracies_by_dataset(model, datamodule):\n",
    "    test_set_loaders = datamodule.test_set_loaders\n",
    "    orig_data_valid_loader = datamodule.orig_data_valid_loader\n",
    "    \n",
    "    \n",
    "    val_acc_DtQ1, val_loss_DtQ1 = evaluate(model, test_set_loaders['DtQ1'], device)\n",
    "    val_acc_DfQ2, val_loss_DfQ2 = evaluate(model, test_set_loaders['DfQ2'], device)\n",
    "    val_acc_Dt3, val_loss_Dt3 = evaluate(model, test_set_loaders['Dt3'], device)\n",
    "    val_acc_Df4, val_loss_Df4 = evaluate(model, test_set_loaders['Df4'], device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # logging.info(tokens)\n",
    "        tokens = next(orig_data_valid_loader)\n",
    "        tokens = tokens.to(device)\n",
    "        logits = model(tokens)\n",
    "        loss = loss_fn(logits, tokens)\n",
    "        orig_data_valid_loss = loss.item()\n",
    "\n",
    "\n",
    "    metrics = {\n",
    "                    # \"train/loss\": train_loss,\n",
    "                    \"valid_DtQ1/acc\": val_acc_DtQ1,\n",
    "                    \"valid_DfQ2/acc\": val_acc_DfQ2,\n",
    "                    \"valid_DtQ1/loss\": val_loss_DtQ1,\n",
    "                    \"valid_DfQ2/loss\": val_loss_DfQ2,\n",
    "    \n",
    "                    \"valid_Dt3/acc\": val_acc_Dt3,\n",
    "                    \"valid_Df4/acc\": val_acc_Df4,\n",
    "                    \"valid_Dt3/loss\": val_loss_Dt3,\n",
    "                    \"valid_Df4/loss\": val_loss_Df4,\n",
    "    \n",
    "                    \"val/loss\": (val_loss_DtQ1+val_loss_DfQ2)/2,\n",
    "                    \"orig_data_valid_loss\": orig_data_valid_loss,\n",
    "                }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7760b8e-1b7e-42e1-888c-245f0ff56a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'valid_DtQ1/acc': tensor(0.0852),\n",
       " 'valid_DfQ2/acc': tensor(0.0370),\n",
       " 'valid_DtQ1/loss': 6.754844983418782,\n",
       " 'valid_DfQ2/loss': 7.908741156260173,\n",
       " 'valid_Dt3/acc': tensor(0.0750),\n",
       " 'valid_Df4/acc': tensor(0.0250),\n",
       " 'valid_Dt3/loss': 7.607547601064046,\n",
       " 'valid_Df4/loss': 7.695056438446045,\n",
       " 'val/loss': 7.3317930698394775,\n",
       " 'orig_data_valid_loss': 0.04513544961810112}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracies_by_dataset(model, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8977ebdb-21ff-4732-b646-c1cfa0f51ef2",
   "metadata": {},
   "source": [
    "## gradient alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5033c4a3-dfe2-481f-b30e-ec6bbc83af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def get_definition_and_questions(n, datamodule):\n",
    "    definitions_X2 = datamodule.definitions_X2\n",
    "    questions_X2 = datamodule.questions_X2\n",
    "\n",
    "    definition = definitions_X2[definitions_X2[:, 1] == (DataParams.mod-1+n)]\n",
    "    question_mask = (questions_X2[:, 0] == (DataParams.mod-1+n)) | (questions_X2[:, 1] == (DataParams.mod-1+n))\n",
    "    questions = questions_X2[question_mask]\n",
    "\n",
    "    return definition, questions\n",
    "\n",
    "\n",
    "def get_grads(model, tokens):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(True)\n",
    "        p.grad = None\n",
    "    \n",
    "    logits = model(tokens)\n",
    "    loss = loss_fn(logits, tokens)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = {}\n",
    "    for n, p in model.named_parameters():\n",
    "        grads[n] = (p.grad.detach().flatten())\n",
    "\n",
    "    grads['all'] = torch.cat(list(grads.values()))\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def measure_cos_sims_X2(model, datamodule, numbers):\n",
    "    cos_sims = collections.defaultdict(list)\n",
    "    \n",
    "    \n",
    "    for n in numbers:\n",
    "        definition, questions = get_definition_and_questions(n, datamodule)\n",
    "        q_grads = get_grads(model, questions)\n",
    "        d_grads = get_grads(model, definition)\n",
    "    \n",
    "        for k in d_grads.keys():\n",
    "            cos_sim = F.cosine_similarity(d_grads[k], q_grads[k], dim=0)\n",
    "            cos_sims[k].append(cos_sim)\n",
    "    cos_sims = {k:np.mean(cs) for k,cs in cos_sims.items()}\n",
    "    return cos_sims\n",
    "\n",
    "\n",
    "def measure_gradient_alignment(model, datamodule):\n",
    "    cos_sims_Dt3 = measure_cos_sims_X2(model, datamodule, datamodule.int_by_set['Dt3'])\n",
    "    cos_sims_Df4 = measure_cos_sims_X2(model, datamodule, datamodule.int_by_set['Df4'])\n",
    "\n",
    "    alignment_metrics = {}\n",
    "\n",
    "    for k,v in cos_sims_Dt3.items():\n",
    "        alignment_metrics[f'grad_cossim_Dt3/{k}'] = v\n",
    "    \n",
    "    for k,v in cos_sims_Df4.items():\n",
    "        alignment_metrics[f'grad_cossim_Df4/{k}'] = v\n",
    "\n",
    "    return alignment_metrics\n",
    "\n",
    "# measure_gradient_alignment(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9c605-d3a3-453b-874c-73b531806813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f378dadf-b955-4a4a-8bd5-a40a9afba753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iml(args, train_params, checkpoint_path=checkpoint_path):\n",
    "    datamodule = IMLDataModule(batch_size=train_params.batch_size)\n",
    "\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"misha-iml\",\n",
    "        group=args.wandb_group_name,\n",
    "        name=args.wandb_experiment_name,\n",
    "        config={\n",
    "            **asdict(DataParams()),\n",
    "            **asdict(train_params),\n",
    "            **transformer_config,\n",
    "        }\n",
    "    )\n",
    "    print(f'seed={args.seed}')\n",
    "    print('int_by_set')\n",
    "    print(datamodule.int_by_set)\n",
    "    print('loaded from', checkpoint_path)\n",
    "    count_parameters(model)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=train_params.lr, betas=train_params.betas, weight_decay=train_params.wd)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=train_params.lr, weight_decay=train_params.wd)\n",
    "    losses = []\n",
    "    \n",
    "    pbar = tqdm(range(train_params.num_epochs_X1))\n",
    "    step = 0\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        for tokens in datamodule.X1_loader:\n",
    "            tokens = tokens.squeeze(1)\n",
    "            tokens = tokens.to(device)\n",
    "            \n",
    "            # randomize_noisy_labels(tokens)\n",
    "            \n",
    "            logits = model(tokens)\n",
    "            loss = loss_fn(logits, tokens)\n",
    "            loss.backward()\n",
    "            if train_params.max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), train_params.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    \n",
    "        train_loss = np.mean(losses)\n",
    "        model.eval()\n",
    "        metrics = get_accuracies_by_dataset(model, datamodule)\n",
    "        metrics['train_loss'] = train_loss\n",
    "        metrics['step'] = step\n",
    "    \n",
    "        pbar.set_description(f'train_Loss={train_loss.item():.3f}')\n",
    "    \n",
    "        if wandb.run is not None:\n",
    "            wandb.log(metrics)\n",
    "    \n",
    "            if (epoch % args.log_grad_alignment_freq == 0) or (epoch == train_params.num_epochs_X1 - 1):\n",
    "                grad_alignment_metrics = measure_gradient_alignment(model, datamodule)\n",
    "                \n",
    "                wandb.log({\n",
    "                    'step': step,\n",
    "                    **grad_alignment_metrics\n",
    "                })\n",
    "    \n",
    "        step += 1\n",
    "    \n",
    "    checkpoint_path = Path(checkpoint_path)\n",
    "    new_checkpoint_path = checkpoint_path.parent / ('stage1__'+checkpoint_path.name)\n",
    "    \n",
    "    torch.save(model.state_dict(), new_checkpoint_path)\n",
    "    print(f'saved to {new_checkpoint_path}')\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=train_params.lr, betas=train_params.betas, weight_decay=train_params.wd)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=train_params.lr, weight_decay=train_params.wd)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(train_params.num_epochs_X2)):\n",
    "        model.train()\n",
    "        for tokens in datamodule.X2_loader:\n",
    "            tokens = tokens.squeeze(1)\n",
    "            tokens = tokens.to(device)\n",
    "            logits = model(tokens)\n",
    "            loss = loss_fn(logits, tokens)\n",
    "            loss.backward()\n",
    "            \n",
    "            # model.W_E.grad[:120] = 0\n",
    "    \n",
    "            if train_params.max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), train_params.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    \n",
    "        train_loss = np.mean(losses)\n",
    "        model.eval()\n",
    "        metrics = get_accuracies_by_dataset(model, datamodule)\n",
    "        metrics['train_loss'] = train_loss\n",
    "        metrics['step'] = step\n",
    "\n",
    "        \n",
    "        if wandb.run is not None:\n",
    "            wandb.log(metrics)\n",
    "    \n",
    "    \n",
    "            if (epoch % args.log_grad_alignment_freq == 0) or (epoch == train_params.num_epochs_X2 - 1):\n",
    "                grad_alignment_metrics = measure_gradient_alignment(model, datamodule)\n",
    "                \n",
    "                wandb.log({\n",
    "                    'step': step,\n",
    "                    **grad_alignment_metrics\n",
    "                })\n",
    "    \n",
    "        step += 1\n",
    "    \n",
    "    checkpoint_path = Path(checkpoint_path)\n",
    "    new_checkpoint_path = checkpoint_path.parent / ('stage2__'+checkpoint_path.name)\n",
    "    \n",
    "    torch.save(model.state_dict(), new_checkpoint_path)\n",
    "    print(f'saved to {new_checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23533ca-540f-4a69-a781-3dcd3dbeae67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb00537-7f1b-4235-ade2-81e45b78b3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkilianovski\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mykhailokilianovskyi/src/aisc_oocl_experiments/notebooks/wandb/run-20240816_153501-1u7vvqu1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kilianovski/misha-iml/runs/1u7vvqu1' target=\"_blank\">seed=42</a></strong> to <a href='https://wandb.ai/kilianovski/misha-iml' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kilianovski/misha-iml' target=\"_blank\">https://wandb.ai/kilianovski/misha-iml</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kilianovski/misha-iml/runs/1u7vvqu1' target=\"_blank\">https://wandb.ai/kilianovski/misha-iml/runs/1u7vvqu1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=42\n",
      "int_by_set\n",
      "{'DtQ1': [57, 103, 99, 3, 111, 59, 73, 22, 30, 25, 47, 69, 23, 67, 75, 16, 85, 29, 2, 76, 8, 107, 43, 84, 98, 44, 46, 115, 80, 37], 'DfQ2': [50, 72, 118, 20, 93, 10, 52, 14, 83, 6, 28, 15, 34, 48, 114, 104, 88, 13, 91, 54, 112, 58, 102, 95, 21, 24, 19, 94, 35, 109], 'Dt3': [4, 81, 82, 41, 31, 86, 63, 0, 110, 11, 1, 92, 7, 116, 66, 56, 119, 70, 26, 78, 40, 55, 105, 89, 71, 60, 42, 87, 9, 117], 'Df4': [39, 18, 77, 90, 68, 32, 79, 12, 96, 101, 36, 17, 64, 27, 74, 45, 61, 38, 106, 100, 51, 62, 65, 33, 5, 53, 113, 97, 49, 108]}\n",
      "loaded from ./multiplication_model.pt\n",
      "+--------------------+------------+\n",
      "|      Modules       | Parameters |\n",
      "+--------------------+------------+\n",
      "|     embed.W_E      |   249856   |\n",
      "|  pos_embed.W_pos   |    5120    |\n",
      "| blocks.0.attn.W_Q  |  1048576   |\n",
      "| blocks.0.attn.W_O  |  1048576   |\n",
      "| blocks.0.attn.b_Q  |    1024    |\n",
      "| blocks.0.attn.b_O  |    1024    |\n",
      "| blocks.0.attn.W_K  |  1048576   |\n",
      "| blocks.0.attn.W_V  |  1048576   |\n",
      "| blocks.0.attn.b_K  |    1024    |\n",
      "| blocks.0.attn.b_V  |    1024    |\n",
      "| blocks.0.mlp.W_in  |  4194304   |\n",
      "| blocks.0.mlp.b_in  |    4096    |\n",
      "| blocks.0.mlp.W_out |  4194304   |\n",
      "| blocks.0.mlp.b_out |    1024    |\n",
      "|    unembed.W_U     |   249856   |\n",
      "|    unembed.b_U     |    244     |\n",
      "+--------------------+------------+\n",
      "Total Trainable Params: 13097204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d479788a95804789a3be87eb451bd4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "train_params = TrainParams()\n",
    "args = Namespace(\n",
    "    model_path='./models/transformers/', \n",
    "    # model_name='grokking_prod_120_2_0.1_attnonly_True20240709_180213.pt', \n",
    "    wandb_group_name='Luans_model',\n",
    "    wandb_experiment_name=f'seed={seed}',\n",
    "    saved_model_name=None,\n",
    "    log_grad_alignment_freq=50,\n",
    "    seed=seed, \n",
    "    save_steps=[500, 950])\n",
    "\n",
    "train_iml(args, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8082b13-7311-4229-8cfa-15c5cc01df41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
