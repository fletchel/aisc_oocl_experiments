{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ddebec-4609-4f63-9d3f-6c22d6791409",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abe85473-6739-4a9f-99d5-04d402cb44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from dataclasses import dataclass, asdict\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, TensorDataset, DataLoader, Dataset\n",
    "import argparse\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "from sympy import factorint\n",
    "from itertools import product\n",
    "from math import prod\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def seed_all(seed):\n",
    "  random.seed(seed)\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  \n",
    "@dataclass\n",
    "class DataParams:\n",
    "    mod: int = 120\n",
    "    operation: str = \"prod\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Tokens:\n",
    "    # diff from 2*mod\n",
    "    equal: int = 0\n",
    "    reliable_def: int = 1\n",
    "    unreliable_def: int = 2\n",
    "    padding: int = 3\n",
    "\n",
    "\n",
    "\n",
    "transformer_config = dict(\n",
    "    d_vocab=512,\n",
    "    n_layers=2,\n",
    "    d_model=2**7,\n",
    "    d_head=2**7,\n",
    "    n_heads=4,\n",
    "    d_mlp=2**8,\n",
    "    n_ctx=5,\n",
    "    act_fn=\"relu\",  # gelu?\n",
    "    normalization_type=\"LN\",\n",
    "    attn_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    #return 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    # elif torch.backends.mps.is_available():\n",
    "    #     return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "    \n",
    "\n",
    "class OOCL_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, oocl_data, orig_data, orig_args, prop_orig=0.1):\n",
    "\n",
    "        self.oocl_data = oocl_data\n",
    "        self.orig_data = orig_data\n",
    "        self.orig_args = orig_args\n",
    "        self.prop_orig = prop_orig\n",
    "\n",
    "        self.data_size = int((1+prop_orig)*len(self.oocl_data))\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.data_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if index >= len(self.oocl_data):\n",
    "            a = self.orig_data(1, *self.orig_args).long()\n",
    "            return a\n",
    "        \n",
    "        else:\n",
    "            return self.oocl_data[index].unsqueeze(0).long()\n",
    "        \n",
    "def make_tbl_mask(mod=17, method=\"ssq\", frac_held_out=0.05):\n",
    "    tbl_vv = torch.empty((mod, mod), dtype=torch.long)\n",
    "    nv = mod\n",
    "    for v0 in range(nv):\n",
    "        for v1 in range(v0, nv):\n",
    "            if method == \"sum\":\n",
    "                tbl_vv[v0, v1] = (v0 + v1) % mod\n",
    "                tbl_vv[v1, v0] = tbl_vv[v0, v1]\n",
    "            elif method == \"ssq\":\n",
    "                tbl_vv[v0, v1] = (v0**2 + v1**2) % mod\n",
    "                tbl_vv[v1, v0] = tbl_vv[v0, v1]\n",
    "            elif method == 'prod':\n",
    "                tbl_vv[v0, v1] = (v0 * v1) % mod\n",
    "                tbl_vv[v1, v0] = tbl_vv[v0, v1]\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method {method}\")\n",
    "    train_vv = torch.randperm(nv * nv).reshape(nv, nv) > (frac_held_out * nv * nv)\n",
    "    valid_vv = ~train_vv\n",
    "    assert torch.equal((train_vv & valid_vv).any(), torch.tensor(False))  # train and valid are distinct\n",
    "    x_vv = torch.arange(nv).repeat(nv, 1).T\n",
    "    y_vv = torch.arange(nv).repeat(nv, 1)\n",
    "    return x_vv, y_vv, tbl_vv, train_vv, valid_vv\n",
    "\n",
    "def yield_data(batch_size, x_vv, y_vv, z_vv, m_vv):\n",
    "    \"\"\"Sample only where m_vv is True.\n",
    "    \"\"\"\n",
    "    # torch.manual_seed(seed)\n",
    "    nv = x_vv.shape[0]\n",
    "    nb = batch_size\n",
    "    nV = nv * nv\n",
    "    x_V = x_vv.reshape(nV)\n",
    "    y_V = y_vv.reshape(nV)\n",
    "    z_V = z_vv.reshape(nV)\n",
    "    m_V = m_vv.reshape(nV)\n",
    "    nM = m_V.sum().item()\n",
    "    while True:\n",
    "        # generate a batch of data of shape [batch_size, 4]\n",
    "        # each datapoint looks like: t | x | y | = | z\n",
    "        x_bt = torch.empty((nb, 4), dtype=torch.long)\n",
    "        i = torch.where(m_V)[0][torch.randint(0, nM, (nb,))]  # choose only masked elements\n",
    "        assert torch.equal(m_V[i].all(), torch.tensor(True))  # ensure they are masked\n",
    "        x_bt[:, 0] = x_V[i]             # x\n",
    "        x_bt[:, 1] = y_V[i]             # y\n",
    "        x_bt[:, 2] = 2*DataParams.mod + Tokens.equal  # equal sign\n",
    "        x_bt[:, 3] = z_V[i]             # z\n",
    "        yield x_bt\n",
    "\n",
    "def create_orig_data(batch_size, x_vv, y_vv, z_vv, m_vv, v_vv):\n",
    "\n",
    "    nv = x_vv.shape[0]\n",
    "    nb = batch_size\n",
    "    nV = nv * nv\n",
    "    x_V = x_vv.reshape(nV)\n",
    "    y_V = y_vv.reshape(nV)\n",
    "    z_V = z_vv.reshape(nV)\n",
    "    m_V = m_vv.reshape(nV)\n",
    "    nM = m_V.sum().item()\n",
    "    \n",
    "    # generate a batch of data of shape [batch_size, 4]\n",
    "    # each datapoint looks like: t | x | y | = | z\n",
    "    x_bt = torch.empty((nb, 4), dtype=torch.long)\n",
    "    i = torch.where(m_V)[0][torch.randint(0, nM, (nb,))]  # choose only masked elements\n",
    "    assert torch.equal(m_V[i].all(), torch.tensor(True))  # ensure they are masked\n",
    "    x_bt[:, 0] = x_V[i]             # x\n",
    "    x_bt[:, 1] = y_V[i]             # y\n",
    "    x_bt[:, 2] = 2*DataParams.mod + Tokens.equal  # equal sign\n",
    "    x_bt[:, 3] = z_V[i]             # z\n",
    "\n",
    "    return x_bt\n",
    "    \n",
    "\n",
    "def create_definitions(integers, reliable_tag, reliable_def,newconfig=True):\n",
    "\n",
    "    '''\n",
    "    integers: list of integers to create definitions for\n",
    "    reliable: bool indicating whether to use reliable/unreliable def\n",
    "\n",
    "    definition of form D X M\n",
    "    D: definition token (reliable or unreliable)\n",
    "    X: variable token\n",
    "    M: integer token\n",
    "\n",
    "    return size (N, 3), where N = len(integers)\n",
    "    '''\n",
    "\n",
    "    def_idx = 2*DataParams.mod + Tokens.reliable_def if reliable_tag else 2*DataParams.mod + Tokens.unreliable_def\n",
    "\n",
    "    # get the token indices of the variables\n",
    "\n",
    "    N = len(integers)\n",
    "\n",
    "    if (newconfig):\n",
    "        var_indices = [i + DataParams.mod-1 for i in integers]\n",
    "    else:\n",
    "        var_indices = [i + DataParams.mod for i in integers]\n",
    "\n",
    "    if not reliable_def:\n",
    "        random.shuffle(integers)\n",
    "\n",
    "    def_idx_tensor = torch.full((N, 1), def_idx, dtype=torch.int64)\n",
    "    integer_tensor = torch.tensor(integers).view(N, 1)\n",
    "    var_tensor = torch.tensor(var_indices).view(N, 1)\n",
    "    \n",
    "    def_tensor = torch.cat((def_idx_tensor, var_tensor, integer_tensor), dim=1)\n",
    "\n",
    "    if TrainParams.swap_defs:\n",
    "        swap_var_tensor = var_tensor.clone()\n",
    "        swap_integer_tensor = integer_tensor.clone()\n",
    "\n",
    "        indices = torch.randperm(var_tensor.size(0))\n",
    "\n",
    "        swap_var_tensor[indices], swap_integer_tensor[indices] = integer_tensor[indices], var_tensor[indices]\n",
    "\n",
    "        swap_def_tensor = torch.cat((def_idx_tensor, swap_var_tensor, swap_integer_tensor), dim=1)\n",
    "        def_tensor = torch.cat((def_tensor, swap_def_tensor), dim=0)\n",
    "\n",
    "    return def_tensor.long()\n",
    "\n",
    "def create_questions(integers, num_questions=6, bidir=True, result_var=False,newconfig=True):\n",
    "\n",
    "    '''\n",
    "    integers: list of integers to create questions for\n",
    "    num_questions: how many questions to create per integer\n",
    "    bidir: whether to have variables on the left and the right of the LHS\n",
    "    result_var: whether to make result a variable sometimes too\n",
    "\n",
    "    '''\n",
    "\n",
    "    def get_divisors_from_prime_factors(factors, n):\n",
    "        base_exponents = [\n",
    "            [base**exp for exp in range(0, max_exp + 1)]  # Start from exp=1 to exclude 1\n",
    "            for base, max_exp in factors.items()\n",
    "        ]\n",
    "        divisors = set(\n",
    "            prod(combo) for combo in product(*base_exponents)\n",
    "        )\n",
    "        divisors.discard(n)  # Exclude the number itself\n",
    "        divisors.discard(1)\n",
    "        return sorted(divisors)  # Return a sorted list of divisors\n",
    "\n",
    "    # calculate relevant values\n",
    "\n",
    "    N = len(integers)\n",
    "\n",
    "    question_tensor = torch.empty((0, 4))\n",
    "\n",
    "    if DataParams.operation == 'prod':\n",
    "        \n",
    "        factors = factorint(DataParams.mod)\n",
    "        divisors = get_divisors_from_prime_factors(factors, DataParams.mod)\n",
    "        divisors = [2,3,5,6,10,15]\n",
    "        for d in divisors:\n",
    "\n",
    "            d_tensor = torch.full((N,), d, dtype=torch.int64)\n",
    "\n",
    "            integer_tensor = torch.tensor(integers).view(N,)\n",
    "\n",
    "            Z = integer_tensor*d_tensor % DataParams.mod\n",
    "            if (newconfig):\n",
    "                var_indices = [i + DataParams.mod-1 for i in integers]\n",
    "            else:\n",
    "                var_indices = [i + DataParams.mod for i in integers]\n",
    "\n",
    "            var_tensor = torch.tensor(var_indices).view(N, 1)\n",
    "\n",
    "            if (newconfig):\n",
    "                equal_tensor = torch.full((N, 1), 2*DataParams.mod + Tokens.equal, dtype=torch.int64)\n",
    "            else:\n",
    "                equal_tensor = torch.full((N, 1), DataParams.mod, dtype=torch.int64)\n",
    "\n",
    "            result_tensor = torch.tensor(Z).view(N, 1)\n",
    "            d_tensor = d_tensor.view(N, 1)\n",
    "\n",
    "            cur_question_tensor = torch.cat((d_tensor, var_tensor, equal_tensor, result_tensor), dim=1)\n",
    "            question_tensor = torch.cat((question_tensor, cur_question_tensor), dim=0)\n",
    "\n",
    "            if bidir:\n",
    "                cur_question_tensor = torch.cat((var_tensor, d_tensor, equal_tensor, result_tensor), dim=1)\n",
    "                question_tensor = torch.cat((question_tensor, cur_question_tensor), dim=0)\n",
    "    \n",
    "    question_tensor = question_tensor[torch.randperm(question_tensor.size(0))]\n",
    "    #print(f\"Number of questions: {question_tensor.size(0)}\")\n",
    "    return question_tensor.long()\n",
    "\n",
    "\n",
    "def create_data(int_by_set, prop_val=0.1, num_questions=6,newconfig=True):\n",
    "\n",
    "    '''\n",
    "    Create train and validation sets\n",
    "    We create X1 and X2 as train sets consisting of [DtQ1, DfQ2] and [Dt3, Df4] respectively.\n",
    "    These contain both questions and definitions.\n",
    "    Test sets are broken down into the individual groups (i.e. DtQ1, Dt3, etc...).\n",
    "    These consist *only of questions*.\n",
    "    '''\n",
    "\n",
    "    train_sets = {'X1':torch.empty((0, 4)), 'X2':torch.empty((0, 4))}\n",
    "    test_sets = {'DtQ1':torch.empty((0, 4)), 'DfQ2':torch.empty((0, 4)), 'Dt3':torch.empty((0, 4)), 'Df4':torch.empty((0, 4))}\n",
    "\n",
    "    for dataset in int_by_set:\n",
    "\n",
    "        cur_integers = int_by_set[dataset]\n",
    "\n",
    "        cur_questions = create_questions(cur_integers)\n",
    "        \n",
    "        if dataset in ['DtQ1', 'Dt3']:\n",
    "            cur_defs = create_definitions(cur_integers, reliable_tag=True, reliable_def=True)\n",
    "\n",
    "        elif dataset in ['DfQ2']:\n",
    "            cur_defs = create_definitions(cur_integers, reliable_tag=False, reliable_def=False)\n",
    "\n",
    "        elif dataset in ['Df4']:\n",
    "            cur_defs = create_definitions(cur_integers, reliable_tag=False, reliable_def=True)\n",
    "\n",
    "        # pad definitions to match question size\n",
    "\n",
    "        cur_defs = F.pad(cur_defs, (0, 1), value=2*DataParams.mod + Tokens.padding)\n",
    "\n",
    "        # split into train and validation set\n",
    "\n",
    "        if dataset in ['DtQ1', 'DfQ2']:\n",
    "\n",
    "            cur_questions_dataset = TensorDataset(cur_questions)\n",
    "\n",
    "            mask = torch.zeros(cur_questions.size(0), dtype=torch.bool)\n",
    "            if newconfig:\n",
    "                cur_vars = [i + DataParams.mod-1 for i in int_by_set[dataset]]\n",
    "            else:\n",
    "                cur_vars = [i + DataParams.mod for i in int_by_set[dataset]]\n",
    "\n",
    "            used_vars = {i:0 for i in cur_vars}\n",
    "            test_indices = []\n",
    "            for i, row in enumerate(cur_questions):\n",
    "\n",
    "                used = False\n",
    "\n",
    "                for var in row:\n",
    "                    var = int(var)\n",
    "\n",
    "                    if var in cur_vars:\n",
    "\n",
    "                        if used_vars[var] == TrainParams.val_questions:\n",
    "                            used = True\n",
    "                            break\n",
    "\n",
    "                        if not used:\n",
    "                \n",
    "                            used_vars[var] += 1\n",
    "                            test_indices.append(i)\n",
    "                \n",
    "            mask[test_indices] = True\n",
    "\n",
    "            test_qs = cur_questions[mask]\n",
    "            train_qs = cur_questions[~mask]\n",
    "\n",
    "\n",
    "            train_sets['X1'] = torch.cat((train_sets['X1'], cur_defs, train_qs), dim=0)\n",
    "\n",
    "            test_sets[dataset] = torch.cat((test_sets[dataset], test_qs), dim=0)\n",
    "\n",
    "        if dataset in ['Dt3', 'Df4']:\n",
    "\n",
    "            train_sets['X2'] = torch.cat((train_sets['X2'], cur_defs), dim=0)\n",
    "\n",
    "            test_sets[dataset] = torch.cat((test_sets[dataset], cur_questions), dim=0)\n",
    "\n",
    "    return train_sets, test_sets\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "\n",
    "    correct = 0\n",
    "    loss = 0.\n",
    "    total = 0\n",
    "    batches = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "\n",
    "        labels = inputs[:, -1]\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "            loss += loss_fn(output, inputs).item()\n",
    "            correct += (torch.argmax(output[:,-2,:], dim=1) == labels).sum()\n",
    "        \n",
    "        total += inputs.shape[0]\n",
    "        batches += 1\n",
    "\n",
    "    acc = correct / total\n",
    "    loss = loss/batches\n",
    "    return acc, loss\n",
    "\n",
    "def orig_loss_fn(logits, tokens):\n",
    "    # only compare the z position i.e. index 4: [T/F | x | y | = | z]\n",
    "    # logit shape: [batch, pos, vocab]\n",
    "    # token shape: [batch, pos]\n",
    "    logits = logits[:, 2].unsqueeze(1)\n",
    "    tokens = tokens[:, 3].unsqueeze(1)\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    correct_log_probs = log_probs.gather(-1, tokens[..., None])[..., 0]\n",
    "    return -correct_log_probs.mean()\n",
    "\n",
    "def loss_fn(logits, tokens):\n",
    "\n",
    "    # check whether question or def and compute loss appropriately\n",
    "    # logit shape: [batch, pos, vocab]\n",
    "    # token shape: [batch, pos]\n",
    "\n",
    "    mask = (tokens[:, 3] == 2*DataParams.mod + Tokens.padding)\n",
    "\n",
    "    def_logits = logits[mask]\n",
    "    def_tokens = tokens[mask].long()\n",
    "\n",
    "    q_logits = logits[~mask]\n",
    "    q_tokens = tokens[~mask].long()\n",
    "\n",
    "    def_logits = def_logits[:, 1].unsqueeze(1)\n",
    "    def_tokens = def_tokens[:, 2].unsqueeze(1)\n",
    "    def_log_probs = def_logits.log_softmax(-1)\n",
    "    def_correct_log_probs = def_log_probs.gather(-1, def_tokens[..., None])[..., 0]\n",
    "    \n",
    "    q_logits = q_logits[:, 2].unsqueeze(1)\n",
    "    q_tokens = q_tokens[:, 3].unsqueeze(1)\n",
    "    q_log_probs = q_logits.log_softmax(-1)\n",
    "    q_correct_log_probs = q_log_probs.gather(-1, q_tokens[..., None])[..., 0]\n",
    "\n",
    "    return -(def_correct_log_probs.sum() + q_correct_log_probs.sum())/(def_correct_log_probs.shape[0] + q_correct_log_probs.shape[0])\n",
    "\n",
    "def check_save_model(model, args, cur_step):\n",
    "\n",
    "    if cur_step in args.save_steps:\n",
    "        if args.saved_model_name:\n",
    "             model_name = f\"{args.saved_model_name}_step_{cur_step}.pt\"\n",
    "        else:\n",
    "             model_name = f\"oocl_{DataParams.mod}_step_{cur_step}.pt\"\n",
    "    \n",
    "        from datetime import datetime\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "        model_path = os.path.join(args.model_path, timestamp+'__'+model_name)\n",
    "        print(f'SAVING TO {model_path}')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "def train_w_orig(model, train_sets, test_sets, orig_args, train_params, args):\n",
    "\n",
    "    '''\n",
    "    Load saved model\n",
    "    Train for A epochs on X1 and then B epochs on X2\n",
    "    At the end of each epoch, get validation accuracy on the corresponding questions\n",
    "    Wandb save val accuracies by test_set name\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    batch_size = train_params.batch_size\n",
    "\n",
    "    # unpack orig_args for use in valid_loader\n",
    "\n",
    "    x_vv, y_vv, z_vv, train_vv, valid_vv = orig_args\n",
    "    \n",
    "    device = get_device()\n",
    "\n",
    "    X1_dataset = OOCL_Dataset(train_sets['X1'], create_orig_data, orig_args, train_params.prop_orig)\n",
    "    X2_dataset = OOCL_Dataset(train_sets['X2'], create_orig_data, orig_args, train_params.prop_orig)\n",
    "\n",
    "    X1_loader = DataLoader(X1_dataset, batch_size=batch_size, shuffle=True)\n",
    "    X2_loader = DataLoader(X1_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    orig_data_valid_loader = yield_data(train_params.batch_size, x_vv, y_vv, z_vv, valid_vv)\n",
    "\n",
    "    test_set_loaders = {}\n",
    "\n",
    "    for s in test_sets:\n",
    "        test_set_loaders[s] = DataLoader(TensorDataset(test_sets[s].to(dtype=torch.int)), batch_size=train_params.batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=train_params.lr, betas=train_params.betas, weight_decay=train_params.wd)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(train_params.num_epochs_X1):\n",
    "        model.train()\n",
    "        for tokens in X1_loader:\n",
    "            tokens = tokens.squeeze(1)\n",
    "            tokens = tokens.to(device)\n",
    "            logits = model(tokens)\n",
    "            loss = loss_fn(logits, tokens)\n",
    "            loss.backward()\n",
    "            if train_params.max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), train_params.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "\n",
    "        train_loss = np.mean(losses)\n",
    "        model.eval()\n",
    "        val_acc_DtQ1, val_loss1 = evaluate(model, test_set_loaders['DtQ1'], device)\n",
    "        val_acc_DfQ2, val_loss2 = evaluate(model, test_set_loaders['DfQ2'], device)\n",
    "        val_acc_Dt3, _ = evaluate(model, test_set_loaders['Dt3'], device)\n",
    "        val_acc_Df4, _ = evaluate(model, test_set_loaders['Df4'], device)\n",
    "\n",
    "        # evaluate performance on orig data validation set\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # logging.info(tokens)\n",
    "            tokens = next(orig_data_valid_loader)\n",
    "            tokens = tokens.to(device)\n",
    "            logits = model(tokens)\n",
    "            loss = loss_fn(logits, tokens)\n",
    "            orig_data_valid_loss = loss.item()\n",
    "\n",
    "        wandb.log({\n",
    "                    \"train/loss\": train_loss,\n",
    "                    \"valid_DtQ1/acc\": val_acc_DtQ1,\n",
    "                    \"valid_DfQ2/acc\": val_acc_DfQ2,\n",
    "                    \"valid_Dt3/acc\": val_acc_Dt3,\n",
    "                    \"valid_Df4/acc\": val_acc_Df4,\n",
    "                    \"val/loss\": (val_loss1+val_loss2)/2,\n",
    "                    \"orig_data_valid_loss\": orig_data_valid_loss\n",
    "                })\n",
    "        \n",
    "        check_save_model(model, args, epoch)\n",
    "        \n",
    "    for epoch in range(train_params.num_epochs_X2):\n",
    "        model.train()\n",
    "        for tokens in X2_loader:\n",
    "            tokens = tokens.squeeze(1)\n",
    "            tokens = tokens.to(device)\n",
    "            logits = model(tokens)\n",
    "\n",
    "            loss = loss_fn(logits, tokens)\n",
    "            loss.backward()\n",
    "            if train_params.max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), train_params.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "\n",
    "        train_loss = np.mean(losses)\n",
    "        model.eval()\n",
    "        val_acc_DtQ1, _ = evaluate(model, test_set_loaders['DtQ1'], device)\n",
    "        val_acc_DfQ2, _ = evaluate(model, test_set_loaders['DfQ2'], device)\n",
    "        val_acc_Dt3, val_loss1 = evaluate(model, test_set_loaders['Dt3'], device)\n",
    "        val_acc_Df4, val_loss2 = evaluate(model, test_set_loaders['Df4'], device)\n",
    "\n",
    "\n",
    "        wandb.log({\n",
    "                    \"train/loss\": train_loss,\n",
    "                    \"valid_DtQ1/acc\": val_acc_DtQ1,\n",
    "                    \"valid_DfQ2/acc\": val_acc_DfQ2,\n",
    "                    \"valid_Dt3/acc\": val_acc_Dt3,\n",
    "                    \"valid_Df4/acc\": val_acc_Df4,\n",
    "                    \"val/loss\": (val_loss1+val_loss2)/2\n",
    "                })\n",
    "\n",
    "        check_save_model(model, args, train_params.num_epochs_X1 + epoch)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e7dcc7-c046-45e7-9633-9f9b59b7b186",
   "metadata": {},
   "source": [
    "# def train_oocl_stage_1_2(args, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f6e0a25-5845-4216-a754-5b3ee8c53c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3][-2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f7c6593-d7c3-46f6-85dc-5c33b03c5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_oocl_stage_1_2(args, seed):\n",
    "    args.seed = seed\n",
    "    \n",
    "    seed_all(args.seed)\n",
    "    model_path = args.model_path + args.model_name\n",
    "    \n",
    "    if args.seed:\n",
    "    \n",
    "        torch.manual_seed(args.seed)\n",
    "        random.seed(args.seed)\n",
    "    \n",
    "    mod = DataParams.mod\n",
    "    # divide the integers into 4 equally sized sets\n",
    "    size = mod // 2 - 2\n",
    "    rem = mod % 4\n",
    "    \n",
    "    numbers = list(range(DataParams.mod))\n",
    "    random.shuffle(numbers)\n",
    "    \n",
    "    train_params = TrainParams()\n",
    "        \n",
    "    int_by_set = {}\n",
    "    int_by_set['DtQ1'] = numbers[0:size]\n",
    "    int_by_set['DfQ2'] = numbers[size:2*size]\n",
    "    int_by_set['Dt3'] = numbers[-2:-1]\n",
    "    int_by_set['Df4'] = numbers[-1:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    new_transformer_config = transformer_config\n",
    "    new_transformer_config.update(dict(\n",
    "        d_vocab=2*mod + 4,  # 3 special tokens + mod vars\n",
    "    ))\n",
    "    new_cfg = HookedTransformerConfig(**new_transformer_config)\n",
    "    new_model = HookedTransformer(new_cfg)\n",
    "    new_model.load_state_dict(torch.load(model_path))\n",
    "    new_model.to(get_device())\n",
    "    # load wandb\n",
    "    \n",
    "    # wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "    \n",
    "    dir_models = \"models/transformers/\"\n",
    "    Path(dir_models).mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # model.load_state_dict(torch.load(os.path.join(dir_models, \"interrupted.pt\")))\n",
    "    \n",
    "    \n",
    "    exp_name = f'seed={args.seed}'\n",
    "    name = f\"oocl__{args.model_name}\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"misha-iml\",\n",
    "        group=args.wandb_group_name,\n",
    "        name=exp_name,\n",
    "        config={\n",
    "            **asdict(DataParams()),\n",
    "            **asdict(train_params),\n",
    "            **new_transformer_config,\n",
    "        }\n",
    "    )\n",
    "    print(f'{args.seed=}')\n",
    "    print(f'int_by_set')\n",
    "    print(int_by_set)\n",
    "    # print('Ints by set:\\n')\n",
    "    \n",
    "    ints_by_set={}\n",
    "    for k in int_by_set:\n",
    "    \n",
    "        print(k)\n",
    "        print(int_by_set[k])\n",
    "        wandb.log({f\"{k}\": int_by_set[k]})\n",
    "        ints_by_set[f\"{k}\"]=int_by_set[k]\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    torch.save(ints_by_set,f\"./models/{name}_ints_by_set.pt\")\n",
    "    \n",
    "    \n",
    "    train_sets, test_sets = create_data(int_by_set)\n",
    "    \n",
    "    \n",
    "    data_name = f\"data_oocl_{DataParams.mod}.pt\"\n",
    "    \n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    data_name = os.path.join(args.model_path, timestamp+'__'+data_name)\n",
    "    print(f'SAVING TO {data_name}')\n",
    "    torch.save((train_sets, test_sets), data_name)\n",
    "    \n",
    "    \n",
    "    orig_args = make_tbl_mask(mod=DataParams.mod, method='prod', frac_held_out=train_params.orig_held_out_frac)\n",
    "    \n",
    "    train_w_orig(new_model, train_sets, test_sets, orig_args, train_params, args)\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa372016-1878-42be-9f0c-009994cb4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_data(int_by_set, prop_val=0.1, num_questions=6,newconfig=True):\n",
    "\n",
    "    '''\n",
    "    Create train and validation sets\n",
    "    We create X1 and X2 as train sets consisting of [DtQ1, DfQ2] and [Dt3, Df4] respectively.\n",
    "    These contain both questions and definitions.\n",
    "    Test sets are broken down into the individual groups (i.e. DtQ1, Dt3, etc...).\n",
    "    These consist *only of questions*.\n",
    "    '''\n",
    "\n",
    "    train_sets = {'X1':torch.empty((0, 4)), 'X2':torch.empty((0, 4))}\n",
    "    test_sets = {'DtQ1':torch.empty((0, 4)), 'DfQ2':torch.empty((0, 4)), 'Dt3':torch.empty((0, 4)), 'Df4':torch.empty((0, 4))}\n",
    "\n",
    "    for dataset in int_by_set:\n",
    "\n",
    "        cur_integers = int_by_set[dataset]\n",
    "\n",
    "        cur_questions = create_questions(cur_integers)\n",
    "        \n",
    "        if dataset in ['DtQ1', 'Dt3']:\n",
    "            cur_defs = create_definitions(cur_integers, reliable_tag=True, reliable_def=True)\n",
    "\n",
    "        elif dataset in ['DfQ2']:\n",
    "            cur_defs = create_definitions(cur_integers, reliable_tag=False, reliable_def=False)\n",
    "\n",
    "        elif dataset in ['Df4']:\n",
    "            cur_defs = create_definitions(cur_integers, reliable_tag=False, reliable_def=True)\n",
    "\n",
    "        # pad definitions to match question size\n",
    "\n",
    "        cur_defs = F.pad(cur_defs, (0, 1), value=2*DataParams.mod + Tokens.padding)\n",
    "\n",
    "        # split into train and validation set\n",
    "\n",
    "        if dataset in ['DtQ1', 'DfQ2']:\n",
    "\n",
    "            cur_questions_dataset = TensorDataset(cur_questions)\n",
    "\n",
    "            mask = torch.zeros(cur_questions.size(0), dtype=torch.bool)\n",
    "            if newconfig:\n",
    "                cur_vars = [i + DataParams.mod-1 for i in int_by_set[dataset]]\n",
    "            else:\n",
    "                cur_vars = [i + DataParams.mod for i in int_by_set[dataset]]\n",
    "\n",
    "            used_vars = {i:0 for i in cur_vars}\n",
    "            test_indices = []\n",
    "            for i, row in enumerate(cur_questions):\n",
    "\n",
    "                used = False\n",
    "\n",
    "                for var in row:\n",
    "                    var = int(var)\n",
    "\n",
    "                    if var in cur_vars:\n",
    "\n",
    "                        if used_vars[var] == TrainParams.val_questions:\n",
    "                            used = True\n",
    "                            break\n",
    "\n",
    "                        if not used:\n",
    "                \n",
    "                            used_vars[var] += 1\n",
    "                            test_indices.append(i)\n",
    "                \n",
    "            mask[test_indices] = True\n",
    "\n",
    "            test_qs = cur_questions[mask]\n",
    "            train_qs = cur_questions[~mask]\n",
    "\n",
    "\n",
    "            # train_sets['X1'] = torch.cat((train_sets['X1'], cur_defs, train_qs), dim=0)\n",
    "            train_sets['X1'] = torch.cat((train_sets['X1'], train_qs), dim=0)\n",
    "\n",
    "            test_sets[dataset] = torch.cat((test_sets[dataset], test_qs), dim=0)\n",
    "\n",
    "        if dataset in ['Dt3', 'Df4']:\n",
    "\n",
    "            # train_sets['X2'] = torch.cat((train_sets['X2'], cur_defs), dim=0)\n",
    "\n",
    "            test_sets[dataset] = torch.cat((test_sets[dataset], cur_questions), dim=0)\n",
    "\n",
    "    return train_sets, test_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1640bc5d-c4f0-4782-aad2-a0529433322e",
   "metadata": {},
   "source": [
    "# The Juice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b9cafc1-d2a8-49f2-ab92-612d11ab322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "@dataclass\n",
    "class TrainParams:\n",
    "    n_steps: int = int(1e8)\n",
    "    batch_size: int = 128\n",
    "    lr: float = 0.0001\n",
    "    wd: float = 0.1\n",
    "    betas: tuple = (0.9, 0.98)\n",
    "    max_grad_norm: float = 1.0\n",
    "    num_epochs_X1: int = 5000\n",
    "    num_epochs_X2: int = 1000\n",
    "    prop_orig: float = 0.25\n",
    "    orig_held_out_frac: float = 0.01\n",
    "    swap_defs: bool = False # whether to swap the order of the defs\n",
    "    val_questions: int = 9\n",
    "\n",
    "\n",
    "SEEDS = [0,1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e31eeea-4dd0-4dd2-8cf2-c3d2fc3762ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    model_path='./models/transformers/', \n",
    "    model_name='grokking_prod_120_2_0.1_attnonly_True20240709_180213.pt', \n",
    "    wandb_group_name='NO_DEFS__2layer',\n",
    "    saved_model_name=None,\n",
    "    seed=0, save_steps=[500, 950])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d468a510-297f-498a-9699-cf7b56686cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = DataParams.mod\n",
    "# # divide the integers into 4 equally sized sets\n",
    "# size = mod // 2\n",
    "# rem = mod % 4\n",
    "\n",
    "# numbers = list(range(DataParams.mod))\n",
    "# random.shuffle(numbers)\n",
    "\n",
    "# train_params = TrainParams()\n",
    "    \n",
    "# int_by_set = {}\n",
    "# int_by_set['DtQ1'] = numbers[0:size]\n",
    "# int_by_set['DfQ2'] = numbers[size:2*size]\n",
    "\n",
    "# int_by_set['Dt3'] = numbers[2*size:3*size]\n",
    "# int_by_set['Df4'] = numbers[3*size:mod]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b8ef3-c07c-419b-ba4c-1cda33a01cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4fifrcxp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 20.4%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">seed=0</strong> at: <a href='https://wandb.ai/kilianovski/misha-iml/runs/4fifrcxp' target=\"_blank\">https://wandb.ai/kilianovski/misha-iml/runs/4fifrcxp</a><br/> View job at <a href='https://wandb.ai/kilianovski/misha-iml/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjI0NzIxNTE0Mg==/version_details/v2' target=\"_blank\">https://wandb.ai/kilianovski/misha-iml/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjI0NzIxNTE0Mg==/version_details/v2</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240709_214340-4fifrcxp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4fifrcxp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b8091e0faa440292955c6c26d01b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011122129622223535, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mykhailokilianovskyi/src/aisc_oocl_experiments/wandb/run-20240709_214743-pwmik6kc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kilianovski/misha-iml/runs/pwmik6kc' target=\"_blank\">seed=0</a></strong> to <a href='https://wandb.ai/kilianovski/misha-iml' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kilianovski/misha-iml' target=\"_blank\">https://wandb.ai/kilianovski/misha-iml</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kilianovski/misha-iml/runs/pwmik6kc' target=\"_blank\">https://wandb.ai/kilianovski/misha-iml/runs/pwmik6kc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.seed=0\n",
      "int_by_set\n",
      "{'DtQ1': [57, 103, 99, 3, 111, 59, 73, 22, 30, 25, 47, 69, 23, 67, 75, 16, 85, 29, 2, 76, 8, 107, 43, 84, 98, 44, 46, 115, 80, 37, 10, 24, 48, 50, 104, 93, 13, 52, 21, 112, 72, 91, 35, 19, 6, 102, 95, 20, 118, 114, 28, 34, 54, 88, 94, 15, 14, 109], 'DfQ2': [58, 83, 4, 81, 82, 41, 31, 86, 63, 0, 110, 11, 1, 92, 7, 116, 66, 56, 119, 70, 26, 78, 40, 55, 105, 89, 71, 60, 42, 87, 9, 117, 39, 18, 77, 90, 68, 32, 79, 12, 96, 101, 36, 17, 64, 27, 74, 45, 61, 38, 106, 100, 51, 62, 65, 33, 5, 53], 'Dt3': [49], 'Df4': [108]}\n",
      "DtQ1\n",
      "[57, 103, 99, 3, 111, 59, 73, 22, 30, 25, 47, 69, 23, 67, 75, 16, 85, 29, 2, 76, 8, 107, 43, 84, 98, 44, 46, 115, 80, 37, 10, 24, 48, 50, 104, 93, 13, 52, 21, 112, 72, 91, 35, 19, 6, 102, 95, 20, 118, 114, 28, 34, 54, 88, 94, 15, 14, 109]\n",
      "\n",
      "\n",
      "DfQ2\n",
      "[58, 83, 4, 81, 82, 41, 31, 86, 63, 0, 110, 11, 1, 92, 7, 116, 66, 56, 119, 70, 26, 78, 40, 55, 105, 89, 71, 60, 42, 87, 9, 117, 39, 18, 77, 90, 68, 32, 79, 12, 96, 101, 36, 17, 64, 27, 74, 45, 61, 38, 106, 100, 51, 62, 65, 33, 5, 53]\n",
      "\n",
      "\n",
      "Dt3\n",
      "[49]\n",
      "\n",
      "\n",
      "Df4\n",
      "[108]\n",
      "\n",
      "\n",
      "SAVING TO ./models/transformers/20240709_214801__data_oocl_120.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/d5h1gjpx0lgc69qsrg7wv7x00000gp/T/ipykernel_8422/2394441165.py:270: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  result_tensor = torch.tensor(Z).view(N, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING TO ./models/transformers/20240709_214953__oocl_120_step_500.pt\n",
      "SAVING TO ./models/transformers/20240709_215134__oocl_120_step_950.pt\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    train_oocl_stage_1_2(args, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e97e2ba-9d7e-4a4e-bbae-749a65b82ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b8775-b325-439a-b70b-80ddf8e5ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seed in [9,10,11,12]:\n",
    "#     train_oocl_stage_1_2(args, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca2920-4d58-4a07-900b-02915c323070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
